#!/usr/bin/env python
import cv_bridge
import cv2.cv as cv
import cv2
import sys
import os
import rospy
import baxter_interface
from std_msgs.msg import String, Header
from sensor_msgs.msg import Image
from stereo_msgs.msg import DisparityImage
import argparse
import numpy as np
os.environ['GLOG_minloglevel'] = '2'
import caffe
import math
from rospy.numpy_msg import numpy_msg
from grasp.msg import vec
import sensor_msgs.point_cloud2 as pc2
from sensor_msgs.msg import PointCloud2

count = 0;
# blob detector
params = cv2.SimpleBlobDetector_Params()
params.minThreshold = 1
params.maxThreshold = 200
params.filterByArea = True
params.minArea = 20
params.maxArea = 224**2
detector = cv2.SimpleBlobDetector(params)
#Background Subtractor
kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))
kernel2 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10))
MOG = cv2.BackgroundSubtractorMOG2(100,50,True)
MOG2 = cv2.BackgroundSubtractorMOG2(100,50,True)
#ConvNet
modelFile  = '/home/birl/ros_ws/src/grasp/caffe/graspDeploy.prototxt'
weightFile = '/home/birl/ros_ws/src/grasp/caffe/caffeGraspTrainX_iter_10000.caffemodel'
graspNet = caffe.Net(modelFile, weightFile, caffe.TEST)
caffe.set_mode_cpu()
depth = np.zeros((480,640))
depth = depth.astype('uint8')
cv2.namedWindow("raw")
cv2.namedWindow("full")

def feat2rect(x,y,w,h,th):
	al = np.arctan2(h,w)
	diag = np.sqrt(w**2+h**2)
	angle = th + al
	cent = np.array((x,y))
	vec = np.array((diag*np.cos(angle)/2, diag*np.sin(angle)/2))
	vec2 = np.array((w*np.cos(th), w*np.sin(th)))
	pt1x,pt1y = cent - vec
	pt3x,pt3y = cent + vec # opposite points
	pt2x,pt2y = np.array((pt1x,pt1y)) + vec2
	pt4x,pt4y = np.array((pt3x,pt3y)) - vec2
	return np.array((int(pt1x),int(pt1y),int(pt2x),int(pt2y),int(pt3x),int(pt3y),int(pt4x),int(pt4y)))

def rect2img(img,points,u,v,name):
	cv2.circle(img,(u,v),3,(255,0,0),-1)
	cv2.line(img,(points[0],points[1]),(points[2],points[3]),(255,0,0),1)
	cv2.line(img,(points[2],points[3]),(points[4],points[5]),(0,255,0),1)
	cv2.line(img,(points[4],points[5]),(points[6],points[7]),(255,0,0),1)
	cv2.line(img,(points[6],points[7]),(points[0],points[1]),(0,255,0),1)
	cv2.imshow(name, img)
	#cv2.waitKey(1)



def stream(cameraTopic,depthTopic):
	rospy.init_node('graspNet')	
	rate = rospy.Rate(60)
	print '1'
	#depthSub = rospy.Subscriber(depthTopic, PointCloud2, callback3, queue_size=1)
	depthSub = rospy.Subscriber(depthTopic, Image, callback2, queue_size=1)
	print '2'
	imageSub = rospy.Subscriber(cameraTopic, Image, callback, queue_size=1) #',buff_size = 2**20
	print("Displaying. Press Ctrl-C to stop...")
	rospy.spin()
	while not rospy.is_shutdown():
		rate.sleep()

def callback3(data):
	#print dir(data)
	gen = pc2.read_points(data,skip_nans=False)
	ar = np.asarray(list(gen))
	print ar
	print np.count_nonzero(~np.isnan(ar))
	b = np.nan_to_num(np.reshape(ar,(640,480,4)))
	c = np.zeros((640,480,1))
	for i in range(0,640):
		for j in range(0,480):
			c[i,j,0] = np.sqrt(c[i,j,0]**2+c[i,j,0]**2+c[i,j,0]**2)
	print ar[10000,:]
	print type(c)
	print c.shape
	d = cv2.normalize(c,0,1,cv2.NORM_MINMAX)
	cv2.imshow('d',d)
	cv2.waitKey(1)

def callback(data):
	#print 'callback'
	streamDisp(data)
	#return 0

def callback2(data):
	#print 'callback2'
	streamDisp2(data)
	#return 0

def streamDisp2(data):
	global depth	
	bridge = cv_bridge.CvBridge()
	try:
		cv_image = bridge.imgmsg_to_cv2(data,desired_encoding='passthrough')
		#print cv_image
		depth = cv_image
	except cv_bridge.CvBridgeError, e:
		print e

	
	

def streamDisp(data):
	global MOG,MOG2, kernel,kernel2,graspNet,depth,count,detector	
	bridge = cv_bridge.CvBridge()
	try:
		cv_image = bridge.imgmsg_to_cv2(data,desired_encoding='bgr8')
	except cv_bridge.CvBridgeError, e:
		print e
	# display image
	mask = MOG.apply(cv_image)
	mask2 = MOG2.apply(depth)
	#cv2.imshow('depth', depth)
	#cv2.waitKey(1)
	count = count+1
	if (count<100):
		print 'training...'
	elif ((count-100)%20 ==0):
		
		#print count
		#go = raw_input('Capture?')
		#print 'Capture?'
		#cv2.waitKey(100000000)
		cv2.waitKey(1)
		#cv2.imshow("d", depth)
		#cv2.waitKey(1)
		rgd = np.copy(cv_image)
		rgd[:,:,0] = depth[:,:]
		#b,g,r = cv2.split(cv_image)
		#print g.shape
		#print depth.shape
		#rgd = cv2.merge((depth,g,r))
		maskAll = cv2.bitwise_and(mask,mask2)
		mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
		mask2 = cv2.morphologyEx(mask2, cv2.MORPH_OPEN, kernel2)
		maskAll = cv2.bitwise_or(mask,mask2)
		rgdM = cv2.bitwise_and(rgd,rgd,mask = mask)
		rgdM2 = cv2.bitwise_and(rgd,rgd,mask = mask2)
		rgdM3 = cv2.bitwise_and(rgd,rgd,mask = maskAll)
		rgdM4 = cv2.bitwise_and(rgdM,rgdM,mask = mask2)
		#cv2.imshow("1", rgdM)
		#cv2.waitKey(1)
		#cv2.imshow("2", rgdM2)
		#cv2.waitKey(1)
		#cv2.imshow("3", rgdM3)
		#cv2.waitKey(1)
		#cv2.imshow("4", rgdM4)
		#cv2.waitKey(1)
		#return 0
		#rgdM = cv2.bitwise_and(rgdM,rgdM,mask = mask2)
		# image is 640x480
		rgdMCrop = rgdM[480-224:480, 320-112:320+112]	
		#depthMCrop = cv2.resize(depthMCrop,(224,224))
		rows, cols, ch = rgdMCrop.shape
		T = np.float32([[1,0,0],[0,1,-55]])
		rgdMCropT = cv2.warpAffine(rgdMCrop,T,(cols,rows))
		imageB = cv2.blur(rgdMCropT,(30,30))
		gray = cv2.cvtColor(imageB,cv2.COLOR_BGR2GRAY)
		ret,thresh = cv2.threshold(gray,10,255,cv2.THRESH_BINARY_INV)
		keypoints = detector.detect(thresh)
		imageC = cv2.drawKeypoints(thresh,keypoints,np.array([]),(0,0,255),cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
		tx = 0
		ty = 0
		if len(keypoints)>0:
			tx = 112-keypoints[0].pt[0]
			ty = 112 - keypoints[0].pt[1]
			T = np.float32([[1,0,tx],[0,1,ty]])
			rgdMCropT = cv2.warpAffine(rgdMCropT,T,(cols,rows))
		cv2.imshow("keypoints", imageC)
		#imageMCrop = cv2.resize(imageMCrop,(224,224))
		#cv2.imshow("Grasp View", depthMCrop)
		#v2.waitKey(2)
		imageMCrop = rgdMCropT
		inImg = (rgdMCropT.transpose((2,0,1))-144.0)/255
		graspNet.blobs['data'].data[...] = inImg
		pred=graspNet.forward()
		x,y,w,h,c2,s2 = np.array(pred.values(), dtype='float').reshape(-1)
		x=int(x*224)
		y=int(y*224)
		w=(w*224)
		h=(h*224)
		#print w,h
		u=int(x+(320-112)-tx)
		v=int(y+(480-224)+55-ty)
		#print x,y,w,h,c2,s2
		th = (np.arctan2(s2,c2))/2
		th1= np.arccos(c2)/2
		th2=np.arcsin(s2)/2
		#print 'angle'
		#print th,th1,th2
		points=feat2rect(u,v,w,h,th)
		full = np.copy(cv_image)
		rect2img(imageMCrop,feat2rect(x,y,w,h,th),x,y,'raw')
		rect2img(full,feat2rect(u,v,w,h,th),u,v,'full')
		
		vecPub = rospy.Publisher("/vec", numpy_msg(vec), queue_size=10)
		featPub = rospy.Publisher("/feature", Image, queue_size=10)
		print 'u v theta'
		print np.float32(u),np.float32(v),th
		vecPub.publish(np.array((np.float32(u),np.float32(v),np.float32(th))))
		feat = cv_image[v-5:v+4,u-5:u+4,:]
		print feat.shape
		featmsg = cv_bridge.CvBridge().cv2_to_imgmsg(feat,"bgr8")
		featPub.publish(featmsg)
		#return 0
		#go = raw_input('continue?')
	#else:
		#print 'waiting'

def main(args):
	rospy.init_node('graspNet') #, anonymous=True)
	MOG = cv2.BackgroundSubtractorMOG2()
	cameraTopic = '/camera/rgb/image_color'
	#depthTopic = '/camera/depth_registered/disparity'
	depthTopic = '/depthView'
	#depthTopic = '/camera/depth_registered/image_raw'
	#depthTopic = '/camera/depth_registered/points'
	stream(cameraTopic,depthTopic)
	print 'complete!'
	cv2.destroyAllWindows()
	return 0

if __name__ == '__main__':
	main(sys.argv)
